
import logging
import os
import pathlib
import platform
if platform.system().lower() == 'windows':
    temp = pathlib.PosixPath
    pathlib.PosixPath = pathlib.WindowsPath
elif platform.system().lower() == 'linux':
    temp = pathlib.WindowsPath
    pathlib.WindowsPath = pathlib.PosixPath
os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"

import torch
import multiprocessing



from configs.config import Config
import gradio as gr
import pathlib

import logging

import os
import sys
from dotenv import load_dotenv

now_dir = os.getcwd()
sys.path.append(now_dir)
load_dotenv()
from infer.modules.vc.modules import VC

import fairseq
import warnings
import shutil


logging.getLogger("numba").setLevel(logging.WARNING)

logger = logging.getLogger(__name__)

tmp = os.path.join(now_dir, "TEMP")
shutil.rmtree(tmp, ignore_errors=True)
shutil.rmtree("%s/runtime/Lib/site-packages/infer_pack" % (now_dir), ignore_errors=True)
shutil.rmtree("%s/runtime/Lib/site-packages/uvr5_pack" % (now_dir), ignore_errors=True)
os.makedirs(tmp, exist_ok=True)
os.makedirs(os.path.join(now_dir, "logs"), exist_ok=True)
os.makedirs(os.path.join(now_dir, "assets/weights"), exist_ok=True)
os.environ["TEMP"] = tmp
warnings.filterwarnings("ignore")
torch.manual_seed(114514)


config = Config()
vc = VC(config)


if config.dml == True:

    def forward_dml(ctx, x, scale):
        ctx.scale = scale
        res = x.clone().detach()
        return res

    fairseq.modules.grad_multiply.GradMultiply.forward = forward_dml
# åˆ¤æ–­æ˜¯å¦æœ‰èƒ½ç”¨æ¥è®­ç»ƒå’ŒåŠ é€Ÿæ¨ç†çš„Nå¡
ngpu = torch.cuda.device_count()
gpu_infos = []
mem = []
if_gpu_ok = False

if torch.cuda.is_available() or ngpu != 0:
    for i in range(ngpu):
        gpu_name = torch.cuda.get_device_name(i)
        if any(
            value in gpu_name.upper()
            for value in [
                "10",
                "16",
                "20",
                "30",
                "40",
                "A2",
                "A3",
                "A4",
                "P4",
                "A50",
                "500",
                "A60",
                "70",
                "80",
                "90",
                "M4",
                "T4",
                "TITAN",
            ]
        ):
            # A10#A100#V100#A40#P40#M40#K80#A4500
            if_gpu_ok = True  # è‡³å°‘æœ‰ä¸€å¼ èƒ½ç”¨çš„Nå¡
            gpu_infos.append("%s\t%s" % (i, gpu_name))
            mem.append(
                int(
                    torch.cuda.get_device_properties(i).total_memory
                    / 1024
                    / 1024
                    / 1024
                    + 0.4
                )
            )
if if_gpu_ok and len(gpu_infos) > 0:
    gpu_info = "\n".join(gpu_infos)
    default_batch_size = min(mem) // 2
else:
    gpu_info = i18n("å¾ˆé—æ†¾æ‚¨è¿™æ²¡æœ‰èƒ½ç”¨çš„æ˜¾å¡æ¥æ”¯æŒæ‚¨è®­ç»ƒ")
    default_batch_size = 1
gpus = "-".join([i[0] for i in gpu_infos])

from gradio.events import Dependency

class ToolButton(gr.Button, gr.components.FormComponent):
    """Small button with single emoji as text, fits inside gradio forms"""

    def __init__(self, **kwargs):
        super().__init__(variant="tool", **kwargs)

    def get_block_name(self):
        return "button"
# import pdb
# pdb.set_trace()
# weight_root = os.getenv("weight_root")
# weight_uvr5_root = os.getenv("weight_uvr5_root")
# index_root = os.getenv("index_root")


config = Config()
vc = VC(config)


thread_count = multiprocessing.cpu_count()

print("Use",thread_count,"cpu cores for computing")

torch.set_num_threads(thread_count)
torch.set_num_interop_threads(thread_count)


device = torch.device("cpu")
if torch.cuda.is_available():
    device = torch.device("cuda", 0)

vc.get_vc('nifeng.pth',0.33,0.33)

def main():
    app = gr.Blocks(css="footer {visibility: hidden}",title="é£æ¡¨Feng Brother")
    with app:
        gr.HTML("<center>"
                "<h1>ğŸŒŠğŸ’•ğŸ¶ å£°éŸ³å…‹éš†ä¹‹å®æ—¶ä½“éªŒ X è®­ç»ƒ&æ¨ç† </h1>"
                "</center>")
        gr.Markdown("## <center>âš¡ åªéœ€3åˆ†é’Ÿè®­ç»ƒï¼Œå¿«é€Ÿå¤åˆ»æ‚¨å–œæ¬¢çš„å£°éŸ³ï¼›åœ¨çº¿ä½“éªŒå…‹éš†é£æ¡¨å¸…æ°”RDä¹‹Feng Brotherçš„å£°éŸ³</center>")
        gr.Markdown("### <center>æ›´å¤šç²¾å½©éŸ³é¢‘åº”ç”¨ï¼Œæ­£åœ¨æŒç»­æ›´æ–°ï½è”ç³»ä½œè€…ï¼šluyao15@baidu.comğŸ’•</center>")

 
        with gr.Tab("ğŸ¶ - å£°éŸ³å…‹éš†-æ¨ç†ä½“éªŒ"):
            gr.Markdown("è¯·å½•åˆ¶æˆ–ä¸Šä¼ ä¸€æ®µâ€œå¹²å‡€â€å£°éŸ³çš„éŸ³é¢‘ï¼Œå¹¶ç‚¹å‡»â€å£°éŸ³å¤åˆ»â€œï¼Œä¼˜å…ˆå¤„ç†éº¦å…‹é£å£°éŸ³")
            with gr.Row():
                with gr.Column():
                    record_audio_prompt = gr.Audio(label='è¯·åœ¨æ­¤ç”¨éº¦å…‹é£å½•åˆ¶æ‚¨å–œæ¬¢çš„å£°éŸ³', source='microphone', interactive=True, type="filepath")
                    upload_audio_prompt = gr.Audio(label='æˆ–è€…ä¸Šä¼ æ‚¨çš„è¯­éŸ³æ–‡ä»¶', source='upload', interactive=True, type="filepath")
                    
                with gr.Column():
                    audio_output = gr.Audio(label="ä¸ºæ‚¨åˆæˆçš„ä¸“å±è¯­éŸ³", elem_id="tts-audio", type="filepath", interactive=False)
                    vc_transform0 = gr.Number(
                                label="å˜è°ƒ(æ•´æ•°, åŠéŸ³æ•°é‡, å‡å…«åº¦12é™å…«åº¦-12)", value=0
                            )
                    text_output_1 = gr.Textbox(label="å£°éŸ³å…‹éš†è¿›åº¦")
                    btn_1 = gr.Button("å£°éŸ³å¤åˆ»", variant="primary")
                    btn_1.click(vc.vc_nifeng,
                              inputs=[record_audio_prompt, upload_audio_prompt, vc_transform0],
                              outputs=[text_output_1, audio_output],api_name="infer_convert")
                    
                    
        with gr.Tab("ğŸ’• - å£°éŸ³å…‹éš†-è®­ç»ƒä½“éªŒ"):
            gr.Markdown("ç°åœ¨å¼€å§‹å¥‡å¦™çš„å£°éŸ³å…‹éš†è®­ç»ƒä½“éªŒä¹‹æ—…å§ï¼ï¼ˆæ•¬è¯·æœŸå¾…......ï¼‰")
            

        gr.Markdown("### <center>æ³¨æ„â—ï¼šè¯·ä¸è¦ç”Ÿæˆä¼šå¯¹ä¸ªäººä»¥åŠç»„ç»‡é€ æˆä¾µå®³çš„å†…å®¹ï¼Œæ­¤ç¨‹åºä»…ä¾›ç§‘ç ”ã€å­¦ä¹ åŠä¸ªäººå¨±ä¹ä½¿ç”¨ï¼Œä½œè€…ä¸å¯¹è½¯ä»¶å…·å¤‡ä»»ä½•æ§åˆ¶åŠ›, ä½¿ç”¨è½¯ä»¶è€…ã€ä¼ æ’­è½¯ä»¶å¯¼å‡ºçš„å£°éŸ³è€…è‡ªè´Ÿå…¨è´£ã€‚</center>")

    app.launch(share=True, server_port=8918, server_name="0.0.0.0")

if __name__ == "__main__":
    # formatter = (
    #     "%(asctime)s %(levelname)s [%(filename)s:%(lineno)d] %(message)s"
    # )
    # logging.basicConfig(format=formatter, level=logging.INFO)
    main()